I"&<h3 id="为什么模型需要解释性">为什么模型需要解释性</h3>

<p>设想这样一个情景，一家信用卡公司需要使用随机森林算法构建一个检测信用卡欺诈的模型。该模型可以根据大量的特征把每一笔交易分为正常交易/欺诈交易。如果模型将一笔交易判定为欺诈，分析师希望知道为什么模型会做出这个判断，例如，每个特征对最终结果的贡献有多大。</p>

<p>或者说如果一个随机森林模型在老旧数据集中预测时符合预期，在新数据集中预测结果却出乎意料。如何检查是哪些特征造成了这种情况呢？</p>

<h3 id="随机森林是一个黑盒子">随机森林是一个黑盒子</h3>

<p>很多关于随机森林和解释性模型的文献都会告诉你解释随机森林近乎是不可能的，因为随机森林是一个典型的黑盒子。没错，一个森林包含了大量树，每棵树随机选择训练数据、特征，想要通过研究每一棵子树全面理解决策过程几乎不可行。另外，即使我们只研究一棵树，也只有在深度和特征数量有限的基础上才有可能。一颗深度为10的树已经拥有上千个节点，用它作为解释性模型也不可行。</p>

<p>另一个研究随机森林内部的方法是计算特征重要性，一个又一个的抽走节点特征，看看模型的能够改变多少，或者计算 “impurity” 的大小（典型的是回归树中的方差和分类树中的基尼系数或熵）。方法有用，却过于原始、静态，对于理解在真实数据中的决策过程作用甚微。</p>

<h3 id="将黑盒子变成白盒子决策路径">将黑盒子变成白盒子：决策路径</h3>

<p>当谈到一棵决策树时，森林中的每棵树都会产生一条『root-leaf』的路径，包含着一系列由特征做出的决策过程。</p>

<p>假设一棵拥有 <em>M</em> 个叶子的决策树将特征空间分成 <em>M</em> 个区域 \(R_m\), 1 ≤ m ≤ M。决策树的定义函数为\(f(x) = \sum\limits_{m=1}^M c_m I(x, R_m)\), <em>M</em> 是叶子的数量（特征空间的若干区域），\(R_m\) 是特征空间的一个区域，\(c_m\) 是对应 <em>m</em> 空间的常数。<em>I</em> 是指示函数（如果\(x \in R_m\)，返回1，反之0）。为了让回归树与 \(R_m\) 的响应变量均值一致，训练树的时候产生 \(c_m\) 进行约束。定义虽然很精确,「决策函数返回正确叶子上的一个值」，却常常忽略顺着信息节点的决策路径这一动态属性。</p>

<h3 id="例子boston-住房数据">例子：Boston 住房数据</h3>

<p>使用<a href="https://archive.ics.uci.edu/ml/datasets/Housing">Boston住房价格数据集</a>，数据集中包含有郊区的住房价格和一系列关键的特征：空气质量（NOX）、离市中心距离（DIST)以及其他种种 - 可以查看页面获取数据集及特征的完整描述。 我们构建了回归决策树（为了便于阅读限制深度为3）来预测住房价格。同样，子树在每个节点都通过限制条件分出若干带值的叶子（换句话说，值可用来预测）。除此之外，我们还画出了内部节点的值（局域相应变量的均值）。</p>

<figure>
<img src="/download/rf.png" alt="image" />
</figure>

<p>这里的亮点是把预测结果给分解开来，沿着预测路径写出了值的变化、特征名字（导致值变的罪魁祸首）。</p>

<p>这个例子希望展示的是另一个方面—预测的运行方式，即与树中的每个节点/决策相关的区域序列。因为每一个决策都是由一个特征控制的，增减属性由母节点提供，预测结果于是可以转换为特征贡献的加和+偏移量（训练集中优势样本的均值）。除去推导，预测函数可以写作：\(f(x) = c_{full} + \sum\limits_{k=1}^K contrib(x, k)\), <em>k</em> 是特征数量，
\(c_{full}\)是根节点的值，\(contrib(x, k)\) 是第 <em>k</em> 个特征在特征向量 <em>x</em> 中的贡献。表面上看起来与线性回归一样简单明了（\(f(x) = a + bx\)）。对于线性回归来说，系数 <em>b</em> 是固定的，用一个常量约束每一个特征的贡献。对于决策树来说，每一个特征的贡献并不是一个能够被预先确定的值，还需要依靠那些确定决策路径的其他特征。</p>

<h3 id="从决策树到森林">从决策树到森林</h3>

<p>我们开始讨论随机森林，如何从决策树转移到随机森林呢？这非常直接，因为森林的预测实际上是所有树预测的均值：\(F(x) = \frac{1}{J} \sum\limits_{j=1}^J f_j(x)\)，<em>J</em>是森林中树的个数。显而易见，对于一个森林，预测结果是偏移的均值与每个特征贡献均值之和：\(F(x) = \frac{1}{J}{\sum\limits_{j=1}^J {c_{j}}_{full}}  + \sum\limits_{k=1}^K (\frac{1}{J}\sum\limits_{j=1}^J contrib_j(x, k))\)。</p>

<h3 id="运行解释器">运行解释器</h3>

<p><em>更新（2015.08.12）</em></p>

<p>可以通过 <a href="https://github.com/andosa/treeinterpreter">treeinterpreter</a> (pip install treeinterpreter) 包来分解 <a href="http://scikit-learn.org/">scikit-learn</a> 中的决策树和随机森林预测结果。更多例子和详细信息可见<a href="http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/">该博客</a>。</p>

<h3 id="总结">总结</h3>

<p>这里提供了一个直接了当的方法让随机森林的预测过程更加具有解释性，甚至能达到线性模型的解释能力 - 是动态而不是静态。每一次预测可以表现为一系列特征的加和，展示这些特征对模型的贡献。这为实用的机器学习和数据科学任务提供了大量的机遇。</p>

<ul>
  <li>向分析者展示一次预测是如何发生的</li>
  <li>当模型与期望不一致可以校正模型</li>
  <li>可以通过比较预测结果均值和相关的特征贡献均值解释两个数据集的差异（例如，治疗前后的行为）</li>
</ul>

<h3 id="reference">Reference</h3>

<ol>
  <li><a href="http://blog.datadive.net/interpreting-random-forests/">Interpreting random forests</a></li>
</ol>

:ET